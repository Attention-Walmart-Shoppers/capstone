{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Walmart Shoppers\n",
    "### A Walmart retail analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was originally retrieved from:\n",
    " -   https://www.kaggle.com/rutuspatel/retail-analysis-with-walmart-sales-data\n",
    " - https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "| Target                  |  Data Type       | Description                     |\n",
    "|-------------------------|------------------|---------------------------------|\n",
    "| next_week_sales_target  |   float64        | Sales in USD per week by store  |\n",
    "\n",
    "\n",
    "| Column Name             |  Data Type        | Description                                                     |  \n",
    "|:------------------------|:------------------|:----------------------------------------------------------------|\n",
    "| store_id                |   int64           | unique identifier for store  (1-45)                             |\n",
    "| Temperature             |   float64         | temperature in Farenheight                                      |\n",
    "| Fuel_Price              |   float64         | cost of fuel(in USD) in region                                  | \n",
    "| CPI                     |   float64         | Prevailing consumer price index, cost of goods                  |\n",
    "| this_week_date          |   datetime64[ns]  | date for current week                                           |\n",
    "| this_week_sales         |   float64         | sales in USD for current week                                   |     \n",
    "| this_week_holiday_flag  |   int64           | indicator of a Holiday for current week (boolean)               |\n",
    "| this_week_unemployment  |   float64         | unemployment rate for current week                              |     \n",
    "| store_type              |   object          | A: SuperStore, B: Walmart, C: neighborhood Walmart              |\n",
    "| store_size              |   int64           | size of specific location in sqft                               |\n",
    "| next_week_1_year_ago    |   float64         | sales for following week of previous year (51 weeks ago)        |\n",
    "| next_week_date          |   datetime64[ns]  | the date of the following week                                  |\n",
    "| next_week_holiday_flag  |   float64         | indicator of a Holiday for following week (boolean)             |\n",
    "| next_week_holiday_name  |   object          | name of holiday for following week                              |  \n",
    "| christmas               |   uint8           | indicator of Christmas (boolean)                                |     \n",
    "| labor_day               |   uint8           | indicator of labor day (boolean)                                |    \n",
    "| pre_christmas           |   uint8           | indicator of pre-christmas: 2 weeks prior to christmas (boolean)|     \n",
    "| super_bowl              |   uint8           | indicator of super bowl (boolean)                               |     \n",
    "| thanksgiving            |   uint8           | indicator of Thanksgiving (boolean)                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "- to predict weekly sales price for a store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think about...\n",
    "- What is your goal?\n",
    "- what is your TARGET? drivers for that target?\n",
    "- what is one oberservation? what does one row from your dataset represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily meetings\n",
    "- standup doc\n",
    "- shared knowledge doc\n",
    "\n",
    "### Three important Questions\n",
    "- what did you work on since we last talked?\n",
    "- what are you planning on working on next?\n",
    "- what are your blockers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#math\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "\n",
    "#custom modules\n",
    "import new_wrangle as w\n",
    "\n",
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in walmart data using new_wrangle.py\n",
    "df= w.acquire_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Holiday_Flag</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>05-02-2010</td>\n",
       "      <td>1643690.90</td>\n",
       "      <td>0</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12-02-2010</td>\n",
       "      <td>1641957.44</td>\n",
       "      <td>1</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19-02-2010</td>\n",
       "      <td>1611968.17</td>\n",
       "      <td>0</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26-02-2010</td>\n",
       "      <td>1409727.59</td>\n",
       "      <td>0</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>05-03-2010</td>\n",
       "      <td>1554806.68</td>\n",
       "      <td>0</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store        Date  Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price  \\\n",
       "0      1  05-02-2010    1643690.90             0        42.31       2.572   \n",
       "1      1  12-02-2010    1641957.44             1        38.51       2.548   \n",
       "2      1  19-02-2010    1611968.17             0        39.93       2.514   \n",
       "3      1  26-02-2010    1409727.59             0        46.63       2.561   \n",
       "4      1  05-03-2010    1554806.68             0        46.50       2.625   \n",
       "\n",
       "          CPI  Unemployment Type    Size  \n",
       "0  211.096358         8.106    A  151315  \n",
       "1  211.242170         8.106    A  151315  \n",
       "2  211.289143         8.106    A  151315  \n",
       "3  211.319643         8.106    A  151315  \n",
       "4  211.350143         8.106    A  151315  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6435 entries, 0 to 6434\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Store         6435 non-null   int64  \n",
      " 1   Date          6435 non-null   object \n",
      " 2   Weekly_Sales  6435 non-null   float64\n",
      " 3   Holiday_Flag  6435 non-null   int64  \n",
      " 4   Temperature   6435 non-null   float64\n",
      " 5   Fuel_Price    6435 non-null   float64\n",
      " 6   CPI           6435 non-null   float64\n",
      " 7   Unemployment  6435 non-null   float64\n",
      " 8   Type          6435 non-null   object \n",
      " 9   Size          6435 non-null   int64  \n",
      "dtypes: float64(5), int64(3), object(2)\n",
      "memory usage: 553.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#check for nulls, dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-14d5232657b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import the cleaned data using new_wrangle.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrangle_walmart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/database-exercises/capstone/new_wrangle.py\u001b[0m in \u001b[0;36mwrangle_walmart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# add seasons columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseason_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# holiday column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/database-exercises/capstone/new_wrangle.py\u001b[0m in \u001b[0;36mseason_column\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    148\u001b[0m     '''\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next_week_season'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_week_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_season\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'this_week_season'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis_week_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_season\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/database-exercises/capstone/new_wrangle.py\u001b[0m in \u001b[0;36mget_season\u001b[0;34m(now_date)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;31m# dummy leap year to allow input X-02-29 (leap day)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     seasons = [('winter', (date(Y,  1,  1),  date(Y,  3, 20))),\n\u001b[0m\u001b[1;32m    130\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0;34m'spring'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0;34m'summer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date' is not defined"
     ]
    }
   ],
   "source": [
    "#import the cleaned data using new_wrangle.py\n",
    "df= w.wrangle_walmart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that all columns are created\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the data\n",
    "df.tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "train, test = w.split_scale(df,'next_week_sales_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of season\n",
    "#train.season.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts by holidays\n",
    "train.next_week_holiday_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bivariate exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average weekly sales by store\n",
    "stores = train.groupby(['store_id']).agg({'next_week_sales_target': ['mean']})\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(stores.index,stores['next_week_sales_target']['mean'])\n",
    "plt.xticks(np.arange(1, 46, step=1))\n",
    "plt.ylabel('Next Week Sales (in USD)', fontsize=16)\n",
    "plt.xlabel('Store', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize store_type by store_size\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='store_type', y='store_size', data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- Store A: appears to be only larger stores\n",
    "- Store B: appear to be midsized stores\n",
    "- Store C: appears to be only smaller stores\n",
    "\n",
    "- outliers were addressed (store 3, store 5, store 33, store 36 were classified incorrectly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize stores and weekly sales\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(x='store_type', y='next_week_sales_target', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize store type and unemployment rate\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(x='store_type', y='this_week_unemployment', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "walmart = train.corr()\n",
    "walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this shows correlation with sales\n",
    "wal_corr = walmart['next_week_sales_target'].sort_values(ascending=False)\n",
    "wal_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(df.corr()[['next_week_sales_target']].sort_values(by='next_week_sales_target', ascending=False), vmin=-1, vmax=1, annot=True, cmap='mako_r')\n",
    "heatmap.set_title('Features Correlating with weekly sales', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Pearson's (cont vs cont)\n",
    "$H_0$: There is no correlation between next week sales and store_size\n",
    "\n",
    "$H_a$: There is a correlation between next week sales and store_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearsons correlation on entire train set\n",
    "#number of rows\n",
    "n = train.shape[0] \n",
    "\n",
    "#degrees of freedom- how much the data can vary\n",
    "deg_f = n-2 \n",
    "\n",
    "#confidence interval (!)\n",
    "conf_in = 0.95\n",
    "\n",
    "alpha = 1- conf_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= train.next_week_sales_target\n",
    "y= train.store_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pearsonr(x,y)\n",
    "r,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We calculate a pearson r of {r:3f} and a statistical certainty p of {p:4f}')\n",
    "print(f'Because p {p:4f} < α  {alpha:4f}, we can reject our null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- We rejected our null hypothesis, thus indicating that there is a correlation between next week sales and store size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 2: Pearson's (cont vs cont)\n",
    "$H_0$: There is no correlation between this week sales and next week sales\n",
    "\n",
    "$H_a$: There is a correlation between this week sales and next week sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearsons correlation on entire train set\n",
    "#number of rows\n",
    "n = train.shape[0] \n",
    "\n",
    "#degrees of freedom- how much the data can vary\n",
    "deg_f = n-2 \n",
    "\n",
    "#confidence interval (!)\n",
    "conf_in = 0.95\n",
    "\n",
    "alpha = 1- conf_in\n",
    "\n",
    "x= train.next_week_sales_target\n",
    "y= train.this_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pearsonr(x,y)\n",
    "r,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We calculate a pearson r of {r:3f} and a statistical certainty p of {p:4f}')\n",
    "print(f'Because p {p:4f} < α  {alpha:4f}, we can reject our null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- We rejected our null hypothesis, thus indicating that there is a correlation between this week sales and next week sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: Pearson's (cont vs cont)\n",
    "$H_0$: There is no correlation between this week sales and sales from this time last year\n",
    "\n",
    "$H_a$: There is a correlation between this week sales and sales from this time last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearsons correlation on entire train set\n",
    "#number of rows\n",
    "n = train.shape[0] \n",
    "\n",
    "#degrees of freedom- how much the data can vary\n",
    "deg_f = n-2 \n",
    "\n",
    "#confidence interval (!)\n",
    "conf_in = 0.95\n",
    "\n",
    "alpha = 1- conf_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= train.next_week_sales_target\n",
    "y= train.next_week_1_year_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.pearsonr(x,y)\n",
    "r,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We calculate a pearson r of {r:3f} and a statistical certainty p of {p:4f}')\n",
    "print(f'Because p {p:4f} < α  {alpha:4f}, we can reject our null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- We rejected our null hypothesis, thus indicating that there is a correlation between this week sales and sales from this week last year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: T-Test (discrete vs cont)\n",
    "$H_0$: There is no relationship between this next_weeks_sales_target and pre_christmas\n",
    "\n",
    "$H_a$: There is a relationship between this next_weeks_sales_target and pre_christmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set alpha\n",
    "alpha = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size, must be more then 30 to meet assumption\n",
    "train.next_week_sales_target.count(), train.pre_christmas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check variance\n",
    "train.next_week_sales_target.var(), train.pre_christmas.var()\n",
    "\n",
    "#this shows not equal varient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-test on entire train set\n",
    "t, p = stats.ttest_ind(train.next_week_sales_target,train.pre_christmas, equal_var=False)\n",
    "t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p <alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We calculate a t of {t:3f} and a statistical certainty p of {p:4f}')\n",
    "print(f'Because p {p:4f} < α  {alpha:4f}, we reject our null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- We rejected our null hypothesis, thus indicating that there is a correlation between next week sales and pre-christmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, X_train_scaled, X_test_scaled, y_train, y_test= w.split_scale(df, 'next_week_sales_target', scaler= MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set features\n",
    "#we do not want to include all columns in this because it could cause overfitting\n",
    "features = ['store_size', 'this_week_unemployment', 'next_week_1_year_ago', 'this_week_sales', 'pre_christmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need y_train and y_validate to be dataframes to append the new columns with predicted values. \n",
    "y_train = pd.DataFrame({'actual': y_train})\n",
    "y_test = pd.DataFrame({'actual': y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the baseline using mean of all sales\n",
    "baseline= y_train['actual'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column called baseline to compare\n",
    "y_train['baseline'] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate RMSE for baseline model\n",
    "rmse_baseline_train= math.sqrt(mean_squared_error(y_train.actual, y_train.baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe to make data easier to visualize/understand\n",
    "metric_df = pd.DataFrame(data=[{\n",
    "    'model': \"Baseline (using mean)\",\n",
    "    'rmse_train': round(rmse_baseline_train, 2),\n",
    "    'r^2_train': round(explained_variance_score(y_train.actual, y_train.baseline),4),\n",
    "\n",
    "}])\n",
    "\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline version 2 using last years sales\n",
    "baseline2 = train['next_week_1_year_ago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "#create column called baseline to compare\n",
    "y_train['last_year_baseline'] = baseline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate RMSE for baseline model\n",
    "rmse_baseline2_train= math.sqrt(mean_squared_error(y_train.actual, y_train.last_year_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe to make data easier to visualize/understand\n",
    "metric_df = metric_df.append(\n",
    "    {\n",
    "    'model': \"Baseline (using last year's sales)\",\n",
    "    'rmse_train': round(rmse_baseline2_train, 2),\n",
    "    'r^2_train': round(explained_variance_score(y_train.actual, y_train.last_year_baseline),4),\n",
    "    }, ignore_index=True)\n",
    "\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinary least squares\n",
    "#create the model \n",
    "model1 = LinearRegression(normalize=True)\n",
    "\n",
    "#fit the model\n",
    "model1.fit(X_train_scaled[features], y_train.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train\n",
    "y_train['sales_pred_lm'] = model1.predict(X_train_scaled[features])\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.actual, y_train.sales_pred_lm)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create visual to see baseline vs LinearRegression model\n",
    "metric_df = metric_df.append(\n",
    "    {\n",
    "    'model': 'Model 1: OLS',\n",
    "    'rmse_train': round(rmse_train, 2),\n",
    "    'r^2_train': round(explained_variance_score(y_train.actual, y_train.sales_pred_lm),4),\n",
    "    }, ignore_index=True)\n",
    "\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Lars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "model2 = LassoLars(alpha= 2)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "model2.fit(X_train_scaled[features], y_train.actual)\n",
    "\n",
    "# predict train\n",
    "y_train['sales_pred_lars'] = model2.predict(X_train_scaled[features])\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.actual, y_train.sales_pred_lars)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows baseline vs LinearRegression vs LassoLars\n",
    "metric_df = metric_df.append(\n",
    "    {\n",
    "    'model': 'Model 2: LassoLars (alpha 2)',\n",
    "    'rmse_train': round(rmse_train,2),\n",
    "    'r^2_train': round(explained_variance_score(y_train.actual, y_train.sales_pred_lars),4),\n",
    "    }, ignore_index=True)\n",
    "\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the polynomial features to get a new set of features\n",
    "model3 = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train_scaled features\n",
    "X_train_degree2 = model3.fit_transform(X_train_scaled[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "lm2 = LinearRegression(normalize=True)\n",
    "\n",
    "#fit the mode\n",
    "lm2.fit(X_train_degree2, y_train.actual)\n",
    "\n",
    "#use the model\n",
    "y_train['sale_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train_model3 = mean_squared_error(y_train.actual, y_train.sale_pred_lm2) ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows baseline vs LinearRegression vs LassoLars\n",
    "metric_df = metric_df.append(\n",
    "    {\n",
    "    'model': 'Model 3: Polynomial Regression (degree=2)',\n",
    "    'rmse_train': round(rmse_train_model3,2),\n",
    "    'r^2_train': round(explained_variance_score(y_train.actual, y_train.sale_pred_lm2),4),\n",
    "    }, ignore_index=True)\n",
    "\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "- Data was scaled using MinMaxScaler\n",
    "- Features included for modeling were: 'store_size', 'this_week_unemployment', 'next_week_1_year_ago', 'this_week_sales', and 'pre_christmas'\n",
    "\n",
    "<br>\n",
    "\n",
    "- 2nd Degree Polynomial Regression model out performed the baseline (using last year's sales) by 23.44% on the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
